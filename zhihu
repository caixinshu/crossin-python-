import requests
import csv

url = "https://www.zhihu.com/api/v3/feed/topstory/recommend?" 
params = {"session_token":"9afd8e821130400cd47a641cee82a493","desktop":"true","page_number":2,"limit":6,"action":"down","after_id":5}
headers = {"user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36",
          "cookie":'_zap=376c470e-f200-478e-803e-2ed6545df385; d_c0="AIBkc1JPAg6PTjTm1PM3xdi5CGcjq2VZHbg=|1533453180"; q_c1=d81d27f605494edba79bd075dbab7d50|1533453181000|1533453181000; _xsrf=tdxRDSEUrvgKVJ5VgVpBl4jsuUAKexto; z_c0=Mi4xSGIyOUFnQUFBQUFBZ0dSelVrOENEaGNBQUFCaEFsVk5Ra0lUWGdEMDFyRnlvRUhJMWM2dDBELUdUWWU2T3B6a3Zn|1562768450|680f58ed831733040548150d721a29d62e62423e; tst=r; tgw_l7_route=a37704a413efa26cf3f23813004f1a3b; unlock_ticket="AACAgMIumQkXAAAAYQJVTQjzcF0IoxkwAfSTwanf2VfoRfufxosK1w=="'}
'''page_number 为2时,after_id为5.
   page_number 为3时,after_id为11
   page_number 为4时,after_id为17
page_number +1  after_id +6
'''
def get_data():
    '''获取知乎数据'''
    try:
        re = requests.get(url=url,headers=headers,params=params
    except error as e:
        print('Not Found')
    else:
        re_json = re.json() 
        data = re_json.get('data') 
        return data

def get_question(item):
    '''获取数据，存在字典里，再将字典添加到列表'''
    question_list = []
    for i in range(len(item)):
        question_dic = {}
        try:
            question_dic['分类'] = item[i].get('action_text')
            question_dic['标题'] = item[i]['target']['question']['title']
            question_dic['作者'] = item[i]['target']['question']['author']['name']
            question_dic['链接'] =  url + '/question/{q_id}/answer/{t_id}'.format(q_id=item[i]['target']['question']['id'],
                                                                                    t_id=item[i]['target']['id'])
        except:
            continue          
        question_list.append(question_dic)
    return question_list

def write_csv(datas):
    '''使用csv的字典写入方法'''
    with open('E:\\test.csv','w+',newline='') as csv_file:
        headers = [k for k in datas[0]]
        writer = csv.DictWriter(csv_file, fieldnames=headers)
        writer.writeheader()
        for data in datas:
            writer.writerow(data)

            
def run():
    item = get_data()
    questions = get_question(item)
    write_csv(questions)
        
run()

==========================分割线===============================


# 4 需登录的网站 - 知乎时间线
import requests as rs
import time, csv

# 通过 Chrome 开发者工具的 network 找到 'https://www.zhihu.com/api/v3/feed/topstory/recommend?session_token' 的链接
# 这个链接是动态的，每个人的链接不一样
url = 'https://www.zhihu.com/api/v3/feed/topstory/recommend?session_token=f5556971c835fd64a90d27aeba900096&desktop=true&page_number=3&limit=6&action=down&after_id=11'

# 登录知乎以后获得 cookie，将 cookie 复制下来保存在此处用于之后的请求
h = {
    'user-agent': 'chrome',
    # 注意：这里 cookie 不是真实的，真实的 cookie 需要你通过 chrome 开发者工具，从要抓取的网页复制过来
    'cookie': '...................._ga=GA1.2.2047845615.1493178880; 899.127;.........................'
}

req = rs.get(url, headers=h)
data_total = req.json()
data_items = data_total['data']  # 时间线的数据列表

with open('zhihu_timeline.csv', 'w', encoding='utf-8') as f:  # 打开 zhihu_timeline.csv 并追加写入
    f_csv = csv.writer(f)  # 将文件对象作为参数传给csv.writer()

    for item in data_items:
        target = item['target']
        target_type = target['type']
        # print(target_type)

        if target_type == 'answer':
            question = target['question']

            line = [
                'answer', item.get('id'), item.get('action_text'),  # 类型，总 id，关键词
                target.get('id'), target.get('voteup_count'), target.get('comment_count'),  # 回答 id，回答点赞数，回答评论数
                target.get('thanks_count'), target.get('visited_count'), target.get('created_time'),
                # 回答感谢数，回答观看数，回答创建时间
                question.get('id'), question.get('title'), question.get('answer_count'),  # 问题id，问题题目，问题回答数
                question.get('follower_count'), question.get('comment_count'), question.get('created')  # 问题关注数，评论数，创建时间
            ]
        elif target_type == 'article':
            line = [
                'article', item.get('id'), item.get('action_text'),  # 类型，总 id，关键词
                target.get('id'), target.get('voteup_count'), target.get('comment_count'),  # 文章 id，文章点赞数，文章评论数
                target.get('title'), target.get('visited_count'), target.get('created'), # 文章题目（文章没有感谢数），文章观看数，文章创建时间
            ]
        else:
            continue

        # print(line)

        f_csv.writerow(line)  # 一次写入一行信息

print('写入完成')
